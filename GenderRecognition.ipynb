{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GenderRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "IpxkNs30bdp7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try: #For executing online from my drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  df = pd.read_csv('/content/drive/My Drive/Datasets/voice.csv')\n",
        "except:  #For executing offline\n",
        "  df = pd.read_csv('voice.csv')"
      ],
      "metadata": {
        "id": "zQYvi1TubaaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d84870-0dcb-40b5-b6b3-c6ed835dd8e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.iloc[:,-1]\n",
        "\n",
        "for i in range(len(y)):\n",
        "  y[i] = 1 if y[i] == 'male' else 0\n",
        "\n",
        "y = pd.DataFrame(np.array(y), columns = ['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLbecldUazM-",
        "outputId": "527838be-b4c7-4c1c-c6f6-0c87584f2150"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Nfi_PHgrl-nf",
        "outputId": "d60d9992-5d40-43e6-b700-2b9fadb1521a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
              "0     0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
              "1     0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
              "2     0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
              "3     0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
              "4     0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
              "...        ...       ...       ...       ...       ...       ...        ...   \n",
              "3163  0.131884  0.084734  0.153707  0.049285  0.201144  0.151859   1.762129   \n",
              "3164  0.116221  0.089221  0.076758  0.042718  0.204911  0.162193   0.693730   \n",
              "3165  0.142056  0.095798  0.183731  0.033424  0.224360  0.190936   1.876502   \n",
              "3166  0.143659  0.090628  0.184976  0.043508  0.219943  0.176435   1.591065   \n",
              "3167  0.165509  0.092884  0.183044  0.070072  0.250827  0.180756   1.705029   \n",
              "\n",
              "             kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
              "0      274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
              "1      634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
              "2     1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
              "3        4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
              "4        4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
              "...           ...       ...       ...  ...       ...       ...       ...   \n",
              "3163     6.630383  0.962934  0.763182  ...  0.131884  0.182790  0.083770   \n",
              "3164     2.503954  0.960716  0.709570  ...  0.116221  0.188980  0.034409   \n",
              "3165     6.604509  0.946854  0.654196  ...  0.142056  0.209918  0.039506   \n",
              "3166     5.388298  0.950436  0.675470  ...  0.143659  0.172375  0.034483   \n",
              "3167     5.769115  0.938829  0.601529  ...  0.165509  0.185607  0.062257   \n",
              "\n",
              "        maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
              "0     0.275862  0.007812  0.007812  0.007812  0.000000  0.000000      1  \n",
              "1     0.250000  0.009014  0.007812  0.054688  0.046875  0.052632      1  \n",
              "2     0.271186  0.007990  0.007812  0.015625  0.007812  0.046512      1  \n",
              "3     0.250000  0.201497  0.007812  0.562500  0.554688  0.247119      1  \n",
              "4     0.266667  0.712812  0.007812  5.484375  5.476562  0.208274      1  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "3163  0.262295  0.832899  0.007812  4.210938  4.203125  0.161929      0  \n",
              "3164  0.275862  0.909856  0.039062  3.679688  3.640625  0.277897      0  \n",
              "3165  0.275862  0.494271  0.007812  2.937500  2.929688  0.194759      0  \n",
              "3166  0.250000  0.791360  0.007812  3.593750  3.585938  0.311002      0  \n",
              "3167  0.271186  0.227022  0.007812  0.554688  0.546875  0.350000      0  \n",
              "\n",
              "[3168 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92ff12a7-a4eb-4621-8dfc-d4a479535ac2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meanfreq</th>\n",
              "      <th>sd</th>\n",
              "      <th>median</th>\n",
              "      <th>Q25</th>\n",
              "      <th>Q75</th>\n",
              "      <th>IQR</th>\n",
              "      <th>skew</th>\n",
              "      <th>kurt</th>\n",
              "      <th>sp.ent</th>\n",
              "      <th>sfm</th>\n",
              "      <th>...</th>\n",
              "      <th>centroid</th>\n",
              "      <th>meanfun</th>\n",
              "      <th>minfun</th>\n",
              "      <th>maxfun</th>\n",
              "      <th>meandom</th>\n",
              "      <th>mindom</th>\n",
              "      <th>maxdom</th>\n",
              "      <th>dfrange</th>\n",
              "      <th>modindx</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.059781</td>\n",
              "      <td>0.064241</td>\n",
              "      <td>0.032027</td>\n",
              "      <td>0.015071</td>\n",
              "      <td>0.090193</td>\n",
              "      <td>0.075122</td>\n",
              "      <td>12.863462</td>\n",
              "      <td>274.402906</td>\n",
              "      <td>0.893369</td>\n",
              "      <td>0.491918</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059781</td>\n",
              "      <td>0.084279</td>\n",
              "      <td>0.015702</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.066009</td>\n",
              "      <td>0.067310</td>\n",
              "      <td>0.040229</td>\n",
              "      <td>0.019414</td>\n",
              "      <td>0.092666</td>\n",
              "      <td>0.073252</td>\n",
              "      <td>22.423285</td>\n",
              "      <td>634.613855</td>\n",
              "      <td>0.892193</td>\n",
              "      <td>0.513724</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066009</td>\n",
              "      <td>0.107937</td>\n",
              "      <td>0.015826</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.009014</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.054688</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.077316</td>\n",
              "      <td>0.083829</td>\n",
              "      <td>0.036718</td>\n",
              "      <td>0.008701</td>\n",
              "      <td>0.131908</td>\n",
              "      <td>0.123207</td>\n",
              "      <td>30.757155</td>\n",
              "      <td>1024.927705</td>\n",
              "      <td>0.846389</td>\n",
              "      <td>0.478905</td>\n",
              "      <td>...</td>\n",
              "      <td>0.077316</td>\n",
              "      <td>0.098706</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.007990</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.046512</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.151228</td>\n",
              "      <td>0.072111</td>\n",
              "      <td>0.158011</td>\n",
              "      <td>0.096582</td>\n",
              "      <td>0.207955</td>\n",
              "      <td>0.111374</td>\n",
              "      <td>1.232831</td>\n",
              "      <td>4.177296</td>\n",
              "      <td>0.963322</td>\n",
              "      <td>0.727232</td>\n",
              "      <td>...</td>\n",
              "      <td>0.151228</td>\n",
              "      <td>0.088965</td>\n",
              "      <td>0.017798</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.201497</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.554688</td>\n",
              "      <td>0.247119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.135120</td>\n",
              "      <td>0.079146</td>\n",
              "      <td>0.124656</td>\n",
              "      <td>0.078720</td>\n",
              "      <td>0.206045</td>\n",
              "      <td>0.127325</td>\n",
              "      <td>1.101174</td>\n",
              "      <td>4.333713</td>\n",
              "      <td>0.971955</td>\n",
              "      <td>0.783568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.135120</td>\n",
              "      <td>0.106398</td>\n",
              "      <td>0.016931</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.712812</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>5.484375</td>\n",
              "      <td>5.476562</td>\n",
              "      <td>0.208274</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3163</th>\n",
              "      <td>0.131884</td>\n",
              "      <td>0.084734</td>\n",
              "      <td>0.153707</td>\n",
              "      <td>0.049285</td>\n",
              "      <td>0.201144</td>\n",
              "      <td>0.151859</td>\n",
              "      <td>1.762129</td>\n",
              "      <td>6.630383</td>\n",
              "      <td>0.962934</td>\n",
              "      <td>0.763182</td>\n",
              "      <td>...</td>\n",
              "      <td>0.131884</td>\n",
              "      <td>0.182790</td>\n",
              "      <td>0.083770</td>\n",
              "      <td>0.262295</td>\n",
              "      <td>0.832899</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>4.210938</td>\n",
              "      <td>4.203125</td>\n",
              "      <td>0.161929</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3164</th>\n",
              "      <td>0.116221</td>\n",
              "      <td>0.089221</td>\n",
              "      <td>0.076758</td>\n",
              "      <td>0.042718</td>\n",
              "      <td>0.204911</td>\n",
              "      <td>0.162193</td>\n",
              "      <td>0.693730</td>\n",
              "      <td>2.503954</td>\n",
              "      <td>0.960716</td>\n",
              "      <td>0.709570</td>\n",
              "      <td>...</td>\n",
              "      <td>0.116221</td>\n",
              "      <td>0.188980</td>\n",
              "      <td>0.034409</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>0.909856</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>3.679688</td>\n",
              "      <td>3.640625</td>\n",
              "      <td>0.277897</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3165</th>\n",
              "      <td>0.142056</td>\n",
              "      <td>0.095798</td>\n",
              "      <td>0.183731</td>\n",
              "      <td>0.033424</td>\n",
              "      <td>0.224360</td>\n",
              "      <td>0.190936</td>\n",
              "      <td>1.876502</td>\n",
              "      <td>6.604509</td>\n",
              "      <td>0.946854</td>\n",
              "      <td>0.654196</td>\n",
              "      <td>...</td>\n",
              "      <td>0.142056</td>\n",
              "      <td>0.209918</td>\n",
              "      <td>0.039506</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>0.494271</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>2.937500</td>\n",
              "      <td>2.929688</td>\n",
              "      <td>0.194759</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3166</th>\n",
              "      <td>0.143659</td>\n",
              "      <td>0.090628</td>\n",
              "      <td>0.184976</td>\n",
              "      <td>0.043508</td>\n",
              "      <td>0.219943</td>\n",
              "      <td>0.176435</td>\n",
              "      <td>1.591065</td>\n",
              "      <td>5.388298</td>\n",
              "      <td>0.950436</td>\n",
              "      <td>0.675470</td>\n",
              "      <td>...</td>\n",
              "      <td>0.143659</td>\n",
              "      <td>0.172375</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.791360</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>3.593750</td>\n",
              "      <td>3.585938</td>\n",
              "      <td>0.311002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3167</th>\n",
              "      <td>0.165509</td>\n",
              "      <td>0.092884</td>\n",
              "      <td>0.183044</td>\n",
              "      <td>0.070072</td>\n",
              "      <td>0.250827</td>\n",
              "      <td>0.180756</td>\n",
              "      <td>1.705029</td>\n",
              "      <td>5.769115</td>\n",
              "      <td>0.938829</td>\n",
              "      <td>0.601529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.165509</td>\n",
              "      <td>0.185607</td>\n",
              "      <td>0.062257</td>\n",
              "      <td>0.271186</td>\n",
              "      <td>0.227022</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.554688</td>\n",
              "      <td>0.546875</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3168 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92ff12a7-a4eb-4621-8dfc-d4a479535ac2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92ff12a7-a4eb-4621-8dfc-d4a479535ac2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92ff12a7-a4eb-4621-8dfc-d4a479535ac2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X = df.drop('label',axis=1)\n",
        "X = StandardScaler().fit(X).transform(X)"
      ],
      "metadata": {
        "id": "Y4mK7tMbax0u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "68be62u2fdP1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. SVM"
      ],
      "metadata": {
        "id": "bwoCTitkqG1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "y=y['label'].astype('int')\n",
        "y_test = y_test['label'].astype('int')\n",
        "svc = SVC(kernel = 'rbf')\n",
        "scores = cross_val_score(svc, X, y, scoring='accuracy', error_score='raise') #cv is cross validation\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXCmJ_vUgkrc",
        "outputId": "5bd18d05-dec6-4113-fbe3-7ea71178b962"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.94006309 0.97003155 0.98107256 0.98894155 0.95734597]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on train set based on cross validation\n",
        "svc.fit(X,y)\n",
        "y_pred = svc.predict(X)\n",
        "print(sum(y_pred==y))\n",
        "print(len(y))\n",
        "print(sum(y_pred==y)/len(y))"
      ],
      "metadata": {
        "id": "OdMIdlhAihPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa117787-4210-41aa-e518-377a141afcd8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3120\n",
            "3168\n",
            "0.9848484848484849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on test set\n",
        "y_test_p= svc.predict(X_test)\n",
        "print(sum(y_test_p==y_test)/len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YeFDmOInEDD",
        "outputId": "f21251e9-e9a4-4d27-fd5e-8f9ac196e40f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9842271293375394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Logistic Regression"
      ],
      "metadata": {
        "id": "T66yhC4cqRHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state = 0).fit(X, y)\n",
        "y_pred = clf.predict(X)\n",
        "print(sum(y == y_pred))"
      ],
      "metadata": {
        "id": "UFzteC2pnIZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f8607e-1eae-4f7a-b7d0-9bb40aa5a0e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clf.coef_)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJx5oImTHD1J",
        "outputId": "5fa517d8-ad68-4915-935a-d196d5e95196"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.37849646e-03  2.89117054e-01 -2.28416805e-01 -8.93470876e-01\n",
            "   5.41574484e-01  1.31585599e+00 -1.28154405e-02 -5.07880731e-01\n",
            "   1.32639498e+00 -1.62474183e+00  2.14043451e-01  2.37849646e-03\n",
            "  -4.99224319e+00  6.41910622e-01 -3.94062509e-02  1.56712939e-02\n",
            "  -3.32948341e-02 -1.50711816e-02 -1.44772382e-02 -3.73784251e-01]]\n",
            "Index(['meanfreq', 'sd', 'median', 'Q25', 'Q75', 'IQR', 'skew', 'kurt',\n",
            "       'sp.ent', 'sfm', 'mode', 'centroid', 'meanfun', 'minfun', 'maxfun',\n",
            "       'meandom', 'mindom', 'maxdom', 'dfrange', 'modindx', 'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test set\n",
        "\n",
        "y_test_pred = clf.predict(X_test)\n",
        "print(\"Accuracy =\", sum(y_test_pred==y_test)/len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbJxDoARpdt4",
        "outputId": "dc3af342-ffe6-4c00-83e6-e6e85c220ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9810725552050473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Neural Network"
      ],
      "metadata": {
        "id": "lLVU3lYIAFhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.linear1=nn.Linear(20, 12)\n",
        "        self.linear2=nn.Linear(12,5)\n",
        "        self.linear3=nn.Linear(5,1)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=F.relu(self.linear1(x))\n",
        "        x=F.relu(self.linear2(x))\n",
        "        x=self.sigmoid(self.linear3(x))\n",
        "        return x\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "EPOCHS = 300\n",
        "net= Net()\n",
        "optm = Adam(net.parameters(), lr = 0.005)\n",
        "\n",
        "def train(model, x, y, optimizer, criterion):\n",
        "    model.zero_grad()\n",
        "    output = model(x)\n",
        "    # print(output.shape)\n",
        "    loss =criterion(output,y.view(-1,1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss, output\n",
        "\n",
        "\n",
        "\n",
        "xt = torch.from_numpy(X_train).float()\n",
        "yt = torch.from_numpy(y_train['label'].to_numpy().astype('int')).float()\n",
        "# loader = DataLoader(df2, batch_size=32, shuffle=True)\n",
        "\n",
        "prevloss=0\n",
        "currloss=0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    loss, predictions = train(net,xt ,yt , optm, criterion)\n",
        "    for idx, i in enumerate(predictions):\n",
        "      i = torch.round(i)\n",
        "      if i == yt[idx]:\n",
        "          correct += 1\n",
        "    acc = (correct/len(X_train))\n",
        "    epoch_loss+=loss\n",
        "    print('Epoch {} Accuracy : {}'.format(epoch+1, acc*100))\n",
        "    print('Epoch {} Loss : {}'.format((epoch+1),epoch_loss))\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "xtest = torch.from_numpy(X_test).float()\n",
        "ytest = torch.from_numpy(y_test.to_numpy().astype('int')).float()\n",
        "pred = net(xtest)\n",
        "pred= torch.round(pred)\n",
        "\n",
        "\n",
        "predi= pred.detach().numpy() \n",
        "\n",
        "predi= predi.reshape(-1)\n",
        "\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"Final test loss-\")\n",
        "print(sum(predi==y_test.to_numpy())/len(y_test))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sr9waa1ts1Is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d795964-930a-4289-fb13-a6d5b5a9aa6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Accuracy : 50.59194948697711\n",
            "Epoch 1 Loss : 0.25746819376945496\n",
            "Epoch 2 Accuracy : 50.74980268350434\n",
            "Epoch 2 Loss : 0.25491729378700256\n",
            "Epoch 3 Accuracy : 50.789265982636145\n",
            "Epoch 3 Loss : 0.2526355981826782\n",
            "Epoch 4 Accuracy : 50.789265982636145\n",
            "Epoch 4 Loss : 0.2505922317504883\n",
            "Epoch 5 Accuracy : 50.789265982636145\n",
            "Epoch 5 Loss : 0.24871966242790222\n",
            "Epoch 6 Accuracy : 50.789265982636145\n",
            "Epoch 6 Loss : 0.2468639463186264\n",
            "Epoch 7 Accuracy : 50.789265982636145\n",
            "Epoch 7 Loss : 0.2449747920036316\n",
            "Epoch 8 Accuracy : 50.789265982636145\n",
            "Epoch 8 Loss : 0.24301421642303467\n",
            "Epoch 9 Accuracy : 50.789265982636145\n",
            "Epoch 9 Loss : 0.24093635380268097\n",
            "Epoch 10 Accuracy : 50.789265982636145\n",
            "Epoch 10 Loss : 0.23871858417987823\n",
            "Epoch 11 Accuracy : 51.10497237569061\n",
            "Epoch 11 Loss : 0.23635336756706238\n",
            "Epoch 12 Accuracy : 52.05209155485399\n",
            "Epoch 12 Loss : 0.2338264286518097\n",
            "Epoch 13 Accuracy : 52.99921073401737\n",
            "Epoch 13 Loss : 0.23114646971225739\n",
            "Epoch 14 Accuracy : 54.73559589581689\n",
            "Epoch 14 Loss : 0.22830966114997864\n",
            "Epoch 15 Accuracy : 57.34017363851618\n",
            "Epoch 15 Loss : 0.22529490292072296\n",
            "Epoch 16 Accuracy : 59.86582478295186\n",
            "Epoch 16 Loss : 0.22207701206207275\n",
            "Epoch 17 Accuracy : 62.82557221783741\n",
            "Epoch 17 Loss : 0.21864452958106995\n",
            "Epoch 18 Accuracy : 65.07498026835043\n",
            "Epoch 18 Loss : 0.21493248641490936\n",
            "Epoch 19 Accuracy : 67.83741120757696\n",
            "Epoch 19 Loss : 0.21092858910560608\n",
            "Epoch 20 Accuracy : 69.96842936069456\n",
            "Epoch 20 Loss : 0.20664215087890625\n",
            "Epoch 21 Accuracy : 73.91475927387529\n",
            "Epoch 21 Loss : 0.202082559466362\n",
            "Epoch 22 Accuracy : 76.1247040252565\n",
            "Epoch 22 Loss : 0.1972799152135849\n",
            "Epoch 23 Accuracy : 78.33464877663772\n",
            "Epoch 23 Loss : 0.19226352870464325\n",
            "Epoch 24 Accuracy : 81.21546961325967\n",
            "Epoch 24 Loss : 0.1870621144771576\n",
            "Epoch 25 Accuracy : 83.18863456985004\n",
            "Epoch 25 Loss : 0.1816888153553009\n",
            "Epoch 26 Accuracy : 85.0828729281768\n",
            "Epoch 26 Loss : 0.17616991698741913\n",
            "Epoch 27 Accuracy : 86.22730860299922\n",
            "Epoch 27 Loss : 0.1705561876296997\n",
            "Epoch 28 Accuracy : 87.01657458563537\n",
            "Epoch 28 Loss : 0.1648830771446228\n",
            "Epoch 29 Accuracy : 87.41120757695343\n",
            "Epoch 29 Loss : 0.15919356048107147\n",
            "Epoch 30 Accuracy : 87.92423046566694\n",
            "Epoch 30 Loss : 0.15356555581092834\n",
            "Epoch 31 Accuracy : 88.67403314917127\n",
            "Epoch 31 Loss : 0.14802443981170654\n",
            "Epoch 32 Accuracy : 88.98973954222573\n",
            "Epoch 32 Loss : 0.14258231222629547\n",
            "Epoch 33 Accuracy : 89.18705603788477\n",
            "Epoch 33 Loss : 0.13724586367607117\n",
            "Epoch 34 Accuracy : 89.77900552486187\n",
            "Epoch 34 Loss : 0.1319878101348877\n",
            "Epoch 35 Accuracy : 90.21310181531176\n",
            "Epoch 35 Loss : 0.12678948044776917\n",
            "Epoch 36 Accuracy : 90.68666140489346\n",
            "Epoch 36 Loss : 0.12164411693811417\n",
            "Epoch 37 Accuracy : 91.00236779794791\n",
            "Epoch 37 Loss : 0.1165335550904274\n",
            "Epoch 38 Accuracy : 91.35753749013418\n",
            "Epoch 38 Loss : 0.11145101487636566\n",
            "Epoch 39 Accuracy : 91.67324388318863\n",
            "Epoch 39 Loss : 0.1063876748085022\n",
            "Epoch 40 Accuracy : 91.91002367797948\n",
            "Epoch 40 Loss : 0.10136187076568604\n",
            "Epoch 41 Accuracy : 92.22573007103394\n",
            "Epoch 41 Loss : 0.09637568145990372\n",
            "Epoch 42 Accuracy : 92.46250986582479\n",
            "Epoch 42 Loss : 0.09148501604795456\n",
            "Epoch 43 Accuracy : 92.97553275453828\n",
            "Epoch 43 Loss : 0.08673211932182312\n",
            "Epoch 44 Accuracy : 93.40962904498816\n",
            "Epoch 44 Loss : 0.08211296051740646\n",
            "Epoch 45 Accuracy : 93.88318863456985\n",
            "Epoch 45 Loss : 0.07763710618019104\n",
            "Epoch 46 Accuracy : 94.43567482241515\n",
            "Epoch 46 Loss : 0.07333748042583466\n",
            "Epoch 47 Accuracy : 94.71191791633781\n",
            "Epoch 47 Loss : 0.06921177357435226\n",
            "Epoch 48 Accuracy : 95.14601420678768\n",
            "Epoch 48 Loss : 0.06530892848968506\n",
            "Epoch 49 Accuracy : 95.58011049723757\n",
            "Epoch 49 Loss : 0.061637040227651596\n",
            "Epoch 50 Accuracy : 95.89581689029202\n",
            "Epoch 50 Loss : 0.05820658430457115\n",
            "Epoch 51 Accuracy : 95.97474348855565\n",
            "Epoch 51 Loss : 0.05501243844628334\n",
            "Epoch 52 Accuracy : 96.01420678768746\n",
            "Epoch 52 Loss : 0.052067287266254425\n",
            "Epoch 53 Accuracy : 96.09313338595106\n",
            "Epoch 53 Loss : 0.049353715032339096\n",
            "Epoch 54 Accuracy : 96.2904498816101\n",
            "Epoch 54 Loss : 0.0468551442027092\n",
            "Epoch 55 Accuracy : 96.25098658247829\n",
            "Epoch 55 Loss : 0.04456344619393349\n",
            "Epoch 56 Accuracy : 96.32991318074191\n",
            "Epoch 56 Loss : 0.042462728917598724\n",
            "Epoch 57 Accuracy : 96.48776637726914\n",
            "Epoch 57 Loss : 0.04054383188486099\n",
            "Epoch 58 Accuracy : 96.52722967640095\n",
            "Epoch 58 Loss : 0.038814980536699295\n",
            "Epoch 59 Accuracy : 96.64561957379637\n",
            "Epoch 59 Loss : 0.0372493751347065\n",
            "Epoch 60 Accuracy : 96.64561957379637\n",
            "Epoch 60 Loss : 0.03583952412009239\n",
            "Epoch 61 Accuracy : 96.64561957379637\n",
            "Epoch 61 Loss : 0.03457114100456238\n",
            "Epoch 62 Accuracy : 96.64561957379637\n",
            "Epoch 62 Loss : 0.03341960534453392\n",
            "Epoch 63 Accuracy : 96.72454617205999\n",
            "Epoch 63 Loss : 0.03237278014421463\n",
            "Epoch 64 Accuracy : 96.8034727703236\n",
            "Epoch 64 Loss : 0.031417228281497955\n",
            "Epoch 65 Accuracy : 96.8429360694554\n",
            "Epoch 65 Loss : 0.03053898550570011\n",
            "Epoch 66 Accuracy : 96.8429360694554\n",
            "Epoch 66 Loss : 0.029730012640357018\n",
            "Epoch 67 Accuracy : 96.8034727703236\n",
            "Epoch 67 Loss : 0.028985461220145226\n",
            "Epoch 68 Accuracy : 96.96132596685084\n",
            "Epoch 68 Loss : 0.02829682268202305\n",
            "Epoch 69 Accuracy : 97.00078926598263\n",
            "Epoch 69 Loss : 0.027658583596348763\n",
            "Epoch 70 Accuracy : 97.07971586424625\n",
            "Epoch 70 Loss : 0.02706269919872284\n",
            "Epoch 71 Accuracy : 97.07971586424625\n",
            "Epoch 71 Loss : 0.02650763839483261\n",
            "Epoch 72 Accuracy : 97.07971586424625\n",
            "Epoch 72 Loss : 0.025991851463913918\n",
            "Epoch 73 Accuracy : 97.19810576164167\n",
            "Epoch 73 Loss : 0.025510214269161224\n",
            "Epoch 74 Accuracy : 97.19810576164167\n",
            "Epoch 74 Loss : 0.025059761479496956\n",
            "Epoch 75 Accuracy : 97.27703235990529\n",
            "Epoch 75 Loss : 0.024638855829834938\n",
            "Epoch 76 Accuracy : 97.31649565903709\n",
            "Epoch 76 Loss : 0.024249400943517685\n",
            "Epoch 77 Accuracy : 97.27703235990529\n",
            "Epoch 77 Loss : 0.023884711787104607\n",
            "Epoch 78 Accuracy : 97.31649565903709\n",
            "Epoch 78 Loss : 0.023541294038295746\n",
            "Epoch 79 Accuracy : 97.3559589581689\n",
            "Epoch 79 Loss : 0.023221394047141075\n",
            "Epoch 80 Accuracy : 97.3559589581689\n",
            "Epoch 80 Loss : 0.02292269468307495\n",
            "Epoch 81 Accuracy : 97.39542225730071\n",
            "Epoch 81 Loss : 0.022644005715847015\n",
            "Epoch 82 Accuracy : 97.3559589581689\n",
            "Epoch 82 Loss : 0.02238268032670021\n",
            "Epoch 83 Accuracy : 97.39542225730071\n",
            "Epoch 83 Loss : 0.022134974598884583\n",
            "Epoch 84 Accuracy : 97.51381215469614\n",
            "Epoch 84 Loss : 0.02189936488866806\n",
            "Epoch 85 Accuracy : 97.55327545382794\n",
            "Epoch 85 Loss : 0.021676672622561455\n",
            "Epoch 86 Accuracy : 97.55327545382794\n",
            "Epoch 86 Loss : 0.021465394645929337\n",
            "Epoch 87 Accuracy : 97.55327545382794\n",
            "Epoch 87 Loss : 0.0212677214294672\n",
            "Epoch 88 Accuracy : 97.59273875295975\n",
            "Epoch 88 Loss : 0.021081367507576942\n",
            "Epoch 89 Accuracy : 97.59273875295975\n",
            "Epoch 89 Loss : 0.02090318873524666\n",
            "Epoch 90 Accuracy : 97.63220205209156\n",
            "Epoch 90 Loss : 0.020733406767249107\n",
            "Epoch 91 Accuracy : 97.63220205209156\n",
            "Epoch 91 Loss : 0.020570160821080208\n",
            "Epoch 92 Accuracy : 97.67166535122335\n",
            "Epoch 92 Loss : 0.020411355420947075\n",
            "Epoch 93 Accuracy : 97.67166535122335\n",
            "Epoch 93 Loss : 0.020256545394659042\n",
            "Epoch 94 Accuracy : 97.71112865035518\n",
            "Epoch 94 Loss : 0.02010856755077839\n",
            "Epoch 95 Accuracy : 97.75059194948697\n",
            "Epoch 95 Loss : 0.019972072914242744\n",
            "Epoch 96 Accuracy : 97.75059194948697\n",
            "Epoch 96 Loss : 0.019840141758322716\n",
            "Epoch 97 Accuracy : 97.75059194948697\n",
            "Epoch 97 Loss : 0.01970927231013775\n",
            "Epoch 98 Accuracy : 97.8295185477506\n",
            "Epoch 98 Loss : 0.01958005130290985\n",
            "Epoch 99 Accuracy : 97.86898184688239\n",
            "Epoch 99 Loss : 0.019451575353741646\n",
            "Epoch 100 Accuracy : 97.86898184688239\n",
            "Epoch 100 Loss : 0.019326092675328255\n",
            "Epoch 101 Accuracy : 97.86898184688239\n",
            "Epoch 101 Loss : 0.019205819815397263\n",
            "Epoch 102 Accuracy : 97.86898184688239\n",
            "Epoch 102 Loss : 0.019086487591266632\n",
            "Epoch 103 Accuracy : 97.90844514601422\n",
            "Epoch 103 Loss : 0.018965037539601326\n",
            "Epoch 104 Accuracy : 97.90844514601422\n",
            "Epoch 104 Loss : 0.018850401043891907\n",
            "Epoch 105 Accuracy : 97.90844514601422\n",
            "Epoch 105 Loss : 0.018737593665719032\n",
            "Epoch 106 Accuracy : 97.90844514601422\n",
            "Epoch 106 Loss : 0.018626516684889793\n",
            "Epoch 107 Accuracy : 97.90844514601422\n",
            "Epoch 107 Loss : 0.018517104908823967\n",
            "Epoch 108 Accuracy : 97.86898184688239\n",
            "Epoch 108 Loss : 0.01840900257229805\n",
            "Epoch 109 Accuracy : 97.90844514601422\n",
            "Epoch 109 Loss : 0.018303556367754936\n",
            "Epoch 110 Accuracy : 97.98737174427782\n",
            "Epoch 110 Loss : 0.01820048689842224\n",
            "Epoch 111 Accuracy : 98.02683504340963\n",
            "Epoch 111 Loss : 0.018099259585142136\n",
            "Epoch 112 Accuracy : 98.02683504340963\n",
            "Epoch 112 Loss : 0.017998356372117996\n",
            "Epoch 113 Accuracy : 98.02683504340963\n",
            "Epoch 113 Loss : 0.01789882406592369\n",
            "Epoch 114 Accuracy : 98.10576164167324\n",
            "Epoch 114 Loss : 0.017802927643060684\n",
            "Epoch 115 Accuracy : 98.10576164167324\n",
            "Epoch 115 Loss : 0.017708394676446915\n",
            "Epoch 116 Accuracy : 98.10576164167324\n",
            "Epoch 116 Loss : 0.017616884782910347\n",
            "Epoch 117 Accuracy : 98.10576164167324\n",
            "Epoch 117 Loss : 0.017527632415294647\n",
            "Epoch 118 Accuracy : 98.10576164167324\n",
            "Epoch 118 Loss : 0.017441285774111748\n",
            "Epoch 119 Accuracy : 98.10576164167324\n",
            "Epoch 119 Loss : 0.017355283722281456\n",
            "Epoch 120 Accuracy : 98.10576164167324\n",
            "Epoch 120 Loss : 0.017269756644964218\n",
            "Epoch 121 Accuracy : 98.10576164167324\n",
            "Epoch 121 Loss : 0.017184758558869362\n",
            "Epoch 122 Accuracy : 98.10576164167324\n",
            "Epoch 122 Loss : 0.01710064895451069\n",
            "Epoch 123 Accuracy : 98.06629834254143\n",
            "Epoch 123 Loss : 0.017015932127833366\n",
            "Epoch 124 Accuracy : 98.06629834254143\n",
            "Epoch 124 Loss : 0.016930432990193367\n",
            "Epoch 125 Accuracy : 98.10576164167324\n",
            "Epoch 125 Loss : 0.01684628799557686\n",
            "Epoch 126 Accuracy : 98.10576164167324\n",
            "Epoch 126 Loss : 0.01676248013973236\n",
            "Epoch 127 Accuracy : 98.10576164167324\n",
            "Epoch 127 Loss : 0.016680318862199783\n",
            "Epoch 128 Accuracy : 98.10576164167324\n",
            "Epoch 128 Loss : 0.016598805785179138\n",
            "Epoch 129 Accuracy : 98.10576164167324\n",
            "Epoch 129 Loss : 0.01651742309331894\n",
            "Epoch 130 Accuracy : 98.10576164167324\n",
            "Epoch 130 Loss : 0.016437353566288948\n",
            "Epoch 131 Accuracy : 98.10576164167324\n",
            "Epoch 131 Loss : 0.016360454261302948\n",
            "Epoch 132 Accuracy : 98.10576164167324\n",
            "Epoch 132 Loss : 0.016284432262182236\n",
            "Epoch 133 Accuracy : 98.10576164167324\n",
            "Epoch 133 Loss : 0.016209838911890984\n",
            "Epoch 134 Accuracy : 98.18468823993686\n",
            "Epoch 134 Loss : 0.01613636128604412\n",
            "Epoch 135 Accuracy : 98.22415153906867\n",
            "Epoch 135 Loss : 0.016063034534454346\n",
            "Epoch 136 Accuracy : 98.22415153906867\n",
            "Epoch 136 Loss : 0.01599016785621643\n",
            "Epoch 137 Accuracy : 98.22415153906867\n",
            "Epoch 137 Loss : 0.015918083488941193\n",
            "Epoch 138 Accuracy : 98.22415153906867\n",
            "Epoch 138 Loss : 0.015846336260437965\n",
            "Epoch 139 Accuracy : 98.22415153906867\n",
            "Epoch 139 Loss : 0.015774905681610107\n",
            "Epoch 140 Accuracy : 98.22415153906867\n",
            "Epoch 140 Loss : 0.01570475473999977\n",
            "Epoch 141 Accuracy : 98.22415153906867\n",
            "Epoch 141 Loss : 0.01563752256333828\n",
            "Epoch 142 Accuracy : 98.22415153906867\n",
            "Epoch 142 Loss : 0.015571562573313713\n",
            "Epoch 143 Accuracy : 98.22415153906867\n",
            "Epoch 143 Loss : 0.015507005155086517\n",
            "Epoch 144 Accuracy : 98.22415153906867\n",
            "Epoch 144 Loss : 0.01544252410531044\n",
            "Epoch 145 Accuracy : 98.22415153906867\n",
            "Epoch 145 Loss : 0.015377785079181194\n",
            "Epoch 146 Accuracy : 98.26361483820048\n",
            "Epoch 146 Loss : 0.015313584357500076\n",
            "Epoch 147 Accuracy : 98.26361483820048\n",
            "Epoch 147 Loss : 0.015251364558935165\n",
            "Epoch 148 Accuracy : 98.26361483820048\n",
            "Epoch 148 Loss : 0.015189106576144695\n",
            "Epoch 149 Accuracy : 98.30307813733228\n",
            "Epoch 149 Loss : 0.015127753838896751\n",
            "Epoch 150 Accuracy : 98.34254143646409\n",
            "Epoch 150 Loss : 0.015067895874381065\n",
            "Epoch 151 Accuracy : 98.34254143646409\n",
            "Epoch 151 Loss : 0.015007664449512959\n",
            "Epoch 152 Accuracy : 98.34254143646409\n",
            "Epoch 152 Loss : 0.01494843140244484\n",
            "Epoch 153 Accuracy : 98.34254143646409\n",
            "Epoch 153 Loss : 0.01488953735679388\n",
            "Epoch 154 Accuracy : 98.34254143646409\n",
            "Epoch 154 Loss : 0.014830445870757103\n",
            "Epoch 155 Accuracy : 98.34254143646409\n",
            "Epoch 155 Loss : 0.014771411195397377\n",
            "Epoch 156 Accuracy : 98.34254143646409\n",
            "Epoch 156 Loss : 0.014712313190102577\n",
            "Epoch 157 Accuracy : 98.34254143646409\n",
            "Epoch 157 Loss : 0.014652947895228863\n",
            "Epoch 158 Accuracy : 98.34254143646409\n",
            "Epoch 158 Loss : 0.014594575390219688\n",
            "Epoch 159 Accuracy : 98.34254143646409\n",
            "Epoch 159 Loss : 0.014537088572978973\n",
            "Epoch 160 Accuracy : 98.34254143646409\n",
            "Epoch 160 Loss : 0.014480048790574074\n",
            "Epoch 161 Accuracy : 98.34254143646409\n",
            "Epoch 161 Loss : 0.014425655826926231\n",
            "Epoch 162 Accuracy : 98.3820047355959\n",
            "Epoch 162 Loss : 0.014371737837791443\n",
            "Epoch 163 Accuracy : 98.3820047355959\n",
            "Epoch 163 Loss : 0.014316431246697903\n",
            "Epoch 164 Accuracy : 98.3820047355959\n",
            "Epoch 164 Loss : 0.01426026877015829\n",
            "Epoch 165 Accuracy : 98.3820047355959\n",
            "Epoch 165 Loss : 0.014205184765160084\n",
            "Epoch 166 Accuracy : 98.4214680347277\n",
            "Epoch 166 Loss : 0.014149892143905163\n",
            "Epoch 167 Accuracy : 98.4214680347277\n",
            "Epoch 167 Loss : 0.014094607904553413\n",
            "Epoch 168 Accuracy : 98.4214680347277\n",
            "Epoch 168 Loss : 0.014040940441191196\n",
            "Epoch 169 Accuracy : 98.4214680347277\n",
            "Epoch 169 Loss : 0.013988180086016655\n",
            "Epoch 170 Accuracy : 98.4214680347277\n",
            "Epoch 170 Loss : 0.01393616572022438\n",
            "Epoch 171 Accuracy : 98.4214680347277\n",
            "Epoch 171 Loss : 0.013883908279240131\n",
            "Epoch 172 Accuracy : 98.4214680347277\n",
            "Epoch 172 Loss : 0.01383233442902565\n",
            "Epoch 173 Accuracy : 98.4214680347277\n",
            "Epoch 173 Loss : 0.013781191781163216\n",
            "Epoch 174 Accuracy : 98.4214680347277\n",
            "Epoch 174 Loss : 0.013731136918067932\n",
            "Epoch 175 Accuracy : 98.4214680347277\n",
            "Epoch 175 Loss : 0.013682709075510502\n",
            "Epoch 176 Accuracy : 98.46093133385952\n",
            "Epoch 176 Loss : 0.01363495085388422\n",
            "Epoch 177 Accuracy : 98.46093133385952\n",
            "Epoch 177 Loss : 0.013586432673037052\n",
            "Epoch 178 Accuracy : 98.46093133385952\n",
            "Epoch 178 Loss : 0.013538453727960587\n",
            "Epoch 179 Accuracy : 98.46093133385952\n",
            "Epoch 179 Loss : 0.013491643592715263\n",
            "Epoch 180 Accuracy : 98.46093133385952\n",
            "Epoch 180 Loss : 0.013446041382849216\n",
            "Epoch 181 Accuracy : 98.50039463299132\n",
            "Epoch 181 Loss : 0.01340007595717907\n",
            "Epoch 182 Accuracy : 98.50039463299132\n",
            "Epoch 182 Loss : 0.013353643007576466\n",
            "Epoch 183 Accuracy : 98.50039463299132\n",
            "Epoch 183 Loss : 0.013307142071425915\n",
            "Epoch 184 Accuracy : 98.50039463299132\n",
            "Epoch 184 Loss : 0.01326180249452591\n",
            "Epoch 185 Accuracy : 98.50039463299132\n",
            "Epoch 185 Loss : 0.01321666780859232\n",
            "Epoch 186 Accuracy : 98.50039463299132\n",
            "Epoch 186 Loss : 0.01317198108881712\n",
            "Epoch 187 Accuracy : 98.50039463299132\n",
            "Epoch 187 Loss : 0.013128500431776047\n",
            "Epoch 188 Accuracy : 98.50039463299132\n",
            "Epoch 188 Loss : 0.013084355741739273\n",
            "Epoch 189 Accuracy : 98.53985793212313\n",
            "Epoch 189 Loss : 0.013041062280535698\n",
            "Epoch 190 Accuracy : 98.53985793212313\n",
            "Epoch 190 Loss : 0.012999163940548897\n",
            "Epoch 191 Accuracy : 98.53985793212313\n",
            "Epoch 191 Loss : 0.01295723207294941\n",
            "Epoch 192 Accuracy : 98.53985793212313\n",
            "Epoch 192 Loss : 0.012915863655507565\n",
            "Epoch 193 Accuracy : 98.57932123125494\n",
            "Epoch 193 Loss : 0.012874896638095379\n",
            "Epoch 194 Accuracy : 98.57932123125494\n",
            "Epoch 194 Loss : 0.012834602035582066\n",
            "Epoch 195 Accuracy : 98.57932123125494\n",
            "Epoch 195 Loss : 0.012795185670256615\n",
            "Epoch 196 Accuracy : 98.57932123125494\n",
            "Epoch 196 Loss : 0.01275634579360485\n",
            "Epoch 197 Accuracy : 98.57932123125494\n",
            "Epoch 197 Loss : 0.012718110345304012\n",
            "Epoch 198 Accuracy : 98.57932123125494\n",
            "Epoch 198 Loss : 0.012679585255682468\n",
            "Epoch 199 Accuracy : 98.57932123125494\n",
            "Epoch 199 Loss : 0.012640836648643017\n",
            "Epoch 200 Accuracy : 98.57932123125494\n",
            "Epoch 200 Loss : 0.012602637521922588\n",
            "Epoch 201 Accuracy : 98.57932123125494\n",
            "Epoch 201 Loss : 0.01256482768803835\n",
            "Epoch 202 Accuracy : 98.57932123125494\n",
            "Epoch 202 Loss : 0.01252775825560093\n",
            "Epoch 203 Accuracy : 98.57932123125494\n",
            "Epoch 203 Loss : 0.012490931898355484\n",
            "Epoch 204 Accuracy : 98.61878453038673\n",
            "Epoch 204 Loss : 0.012454673647880554\n",
            "Epoch 205 Accuracy : 98.61878453038673\n",
            "Epoch 205 Loss : 0.012418271973729134\n",
            "Epoch 206 Accuracy : 98.65824782951856\n",
            "Epoch 206 Loss : 0.012381941080093384\n",
            "Epoch 207 Accuracy : 98.65824782951856\n",
            "Epoch 207 Loss : 0.012345731258392334\n",
            "Epoch 208 Accuracy : 98.65824782951856\n",
            "Epoch 208 Loss : 0.012309788726270199\n",
            "Epoch 209 Accuracy : 98.65824782951856\n",
            "Epoch 209 Loss : 0.012273687869310379\n",
            "Epoch 210 Accuracy : 98.65824782951856\n",
            "Epoch 210 Loss : 0.012237715534865856\n",
            "Epoch 211 Accuracy : 98.65824782951856\n",
            "Epoch 211 Loss : 0.012202853336930275\n",
            "Epoch 212 Accuracy : 98.65824782951856\n",
            "Epoch 212 Loss : 0.012166901491582394\n",
            "Epoch 213 Accuracy : 98.69771112865035\n",
            "Epoch 213 Loss : 0.01213181484490633\n",
            "Epoch 214 Accuracy : 98.69771112865035\n",
            "Epoch 214 Loss : 0.012096972204744816\n",
            "Epoch 215 Accuracy : 98.69771112865035\n",
            "Epoch 215 Loss : 0.012062613852322102\n",
            "Epoch 216 Accuracy : 98.73717442778216\n",
            "Epoch 216 Loss : 0.012028307653963566\n",
            "Epoch 217 Accuracy : 98.73717442778216\n",
            "Epoch 217 Loss : 0.011993810534477234\n",
            "Epoch 218 Accuracy : 98.73717442778216\n",
            "Epoch 218 Loss : 0.011959715746343136\n",
            "Epoch 219 Accuracy : 98.73717442778216\n",
            "Epoch 219 Loss : 0.011925308033823967\n",
            "Epoch 220 Accuracy : 98.73717442778216\n",
            "Epoch 220 Loss : 0.011891407892107964\n",
            "Epoch 221 Accuracy : 98.73717442778216\n",
            "Epoch 221 Loss : 0.011855476535856724\n",
            "Epoch 222 Accuracy : 98.73717442778216\n",
            "Epoch 222 Loss : 0.011819425038993359\n",
            "Epoch 223 Accuracy : 98.73717442778216\n",
            "Epoch 223 Loss : 0.011783634312450886\n",
            "Epoch 224 Accuracy : 98.73717442778216\n",
            "Epoch 224 Loss : 0.011747482232749462\n",
            "Epoch 225 Accuracy : 98.77663772691398\n",
            "Epoch 225 Loss : 0.01171226054430008\n",
            "Epoch 226 Accuracy : 98.77663772691398\n",
            "Epoch 226 Loss : 0.011677034199237823\n",
            "Epoch 227 Accuracy : 98.77663772691398\n",
            "Epoch 227 Loss : 0.011643273755908012\n",
            "Epoch 228 Accuracy : 98.81610102604577\n",
            "Epoch 228 Loss : 0.01160916592925787\n",
            "Epoch 229 Accuracy : 98.81610102604577\n",
            "Epoch 229 Loss : 0.011574613861739635\n",
            "Epoch 230 Accuracy : 98.81610102604577\n",
            "Epoch 230 Loss : 0.011540183797478676\n",
            "Epoch 231 Accuracy : 98.81610102604577\n",
            "Epoch 231 Loss : 0.011505711823701859\n",
            "Epoch 232 Accuracy : 98.81610102604577\n",
            "Epoch 232 Loss : 0.011471830308437347\n",
            "Epoch 233 Accuracy : 98.81610102604577\n",
            "Epoch 233 Loss : 0.011437677778303623\n",
            "Epoch 234 Accuracy : 98.81610102604577\n",
            "Epoch 234 Loss : 0.011403144337236881\n",
            "Epoch 235 Accuracy : 98.81610102604577\n",
            "Epoch 235 Loss : 0.0113690085709095\n",
            "Epoch 236 Accuracy : 98.81610102604577\n",
            "Epoch 236 Loss : 0.011334926821291447\n",
            "Epoch 237 Accuracy : 98.81610102604577\n",
            "Epoch 237 Loss : 0.011301270686089993\n",
            "Epoch 238 Accuracy : 98.81610102604577\n",
            "Epoch 238 Loss : 0.011267922818660736\n",
            "Epoch 239 Accuracy : 98.81610102604577\n",
            "Epoch 239 Loss : 0.011234944686293602\n",
            "Epoch 240 Accuracy : 98.81610102604577\n",
            "Epoch 240 Loss : 0.011202143505215645\n",
            "Epoch 241 Accuracy : 98.81610102604577\n",
            "Epoch 241 Loss : 0.011168964207172394\n",
            "Epoch 242 Accuracy : 98.81610102604577\n",
            "Epoch 242 Loss : 0.011136691085994244\n",
            "Epoch 243 Accuracy : 98.77663772691398\n",
            "Epoch 243 Loss : 0.011104418896138668\n",
            "Epoch 244 Accuracy : 98.77663772691398\n",
            "Epoch 244 Loss : 0.011072360910475254\n",
            "Epoch 245 Accuracy : 98.81610102604577\n",
            "Epoch 245 Loss : 0.011039726436138153\n",
            "Epoch 246 Accuracy : 98.85556432517758\n",
            "Epoch 246 Loss : 0.011007067747414112\n",
            "Epoch 247 Accuracy : 98.85556432517758\n",
            "Epoch 247 Loss : 0.01097496971487999\n",
            "Epoch 248 Accuracy : 98.89502762430939\n",
            "Epoch 248 Loss : 0.010943291708827019\n",
            "Epoch 249 Accuracy : 98.89502762430939\n",
            "Epoch 249 Loss : 0.010911143384873867\n",
            "Epoch 250 Accuracy : 98.89502762430939\n",
            "Epoch 250 Loss : 0.01087877806276083\n",
            "Epoch 251 Accuracy : 98.89502762430939\n",
            "Epoch 251 Loss : 0.010847490280866623\n",
            "Epoch 252 Accuracy : 98.9344909234412\n",
            "Epoch 252 Loss : 0.010815816931426525\n",
            "Epoch 253 Accuracy : 98.9344909234412\n",
            "Epoch 253 Loss : 0.0107831796631217\n",
            "Epoch 254 Accuracy : 98.9344909234412\n",
            "Epoch 254 Loss : 0.010750927962362766\n",
            "Epoch 255 Accuracy : 98.9344909234412\n",
            "Epoch 255 Loss : 0.01071913167834282\n",
            "Epoch 256 Accuracy : 98.9344909234412\n",
            "Epoch 256 Loss : 0.010686931200325489\n",
            "Epoch 257 Accuracy : 98.9344909234412\n",
            "Epoch 257 Loss : 0.010655051097273827\n",
            "Epoch 258 Accuracy : 98.9344909234412\n",
            "Epoch 258 Loss : 0.010623261332511902\n",
            "Epoch 259 Accuracy : 98.9344909234412\n",
            "Epoch 259 Loss : 0.010591705329716206\n",
            "Epoch 260 Accuracy : 98.9344909234412\n",
            "Epoch 260 Loss : 0.010560257360339165\n",
            "Epoch 261 Accuracy : 98.9344909234412\n",
            "Epoch 261 Loss : 0.01052929274737835\n",
            "Epoch 262 Accuracy : 98.9344909234412\n",
            "Epoch 262 Loss : 0.010498222894966602\n",
            "Epoch 263 Accuracy : 98.9344909234412\n",
            "Epoch 263 Loss : 0.01046715211123228\n",
            "Epoch 264 Accuracy : 98.9344909234412\n",
            "Epoch 264 Loss : 0.010436621494591236\n",
            "Epoch 265 Accuracy : 98.9344909234412\n",
            "Epoch 265 Loss : 0.010406611487269402\n",
            "Epoch 266 Accuracy : 98.9344909234412\n",
            "Epoch 266 Loss : 0.010377093218266964\n",
            "Epoch 267 Accuracy : 98.9344909234412\n",
            "Epoch 267 Loss : 0.01034767460078001\n",
            "Epoch 268 Accuracy : 98.9344909234412\n",
            "Epoch 268 Loss : 0.010318171232938766\n",
            "Epoch 269 Accuracy : 98.9344909234412\n",
            "Epoch 269 Loss : 0.010288161225616932\n",
            "Epoch 270 Accuracy : 98.9344909234412\n",
            "Epoch 270 Loss : 0.010257981717586517\n",
            "Epoch 271 Accuracy : 98.9344909234412\n",
            "Epoch 271 Loss : 0.01022818498313427\n",
            "Epoch 272 Accuracy : 98.9344909234412\n",
            "Epoch 272 Loss : 0.010198614560067654\n",
            "Epoch 273 Accuracy : 98.9344909234412\n",
            "Epoch 273 Loss : 0.01016922015696764\n",
            "Epoch 274 Accuracy : 98.9344909234412\n",
            "Epoch 274 Loss : 0.010140511207282543\n",
            "Epoch 275 Accuracy : 98.973954222573\n",
            "Epoch 275 Loss : 0.010115833021700382\n",
            "Epoch 276 Accuracy : 98.973954222573\n",
            "Epoch 276 Loss : 0.01008837390691042\n",
            "Epoch 277 Accuracy : 98.973954222573\n",
            "Epoch 277 Loss : 0.01005883701145649\n",
            "Epoch 278 Accuracy : 98.973954222573\n",
            "Epoch 278 Loss : 0.01003037579357624\n",
            "Epoch 279 Accuracy : 98.973954222573\n",
            "Epoch 279 Loss : 0.010003608651459217\n",
            "Epoch 280 Accuracy : 99.01341752170481\n",
            "Epoch 280 Loss : 0.009977146051824093\n",
            "Epoch 281 Accuracy : 99.01341752170481\n",
            "Epoch 281 Loss : 0.009949914179742336\n",
            "Epoch 282 Accuracy : 99.01341752170481\n",
            "Epoch 282 Loss : 0.009923710487782955\n",
            "Epoch 283 Accuracy : 99.01341752170481\n",
            "Epoch 283 Loss : 0.009896557778120041\n",
            "Epoch 284 Accuracy : 99.01341752170481\n",
            "Epoch 284 Loss : 0.00986997876316309\n",
            "Epoch 285 Accuracy : 99.01341752170481\n",
            "Epoch 285 Loss : 0.009843803942203522\n",
            "Epoch 286 Accuracy : 99.01341752170481\n",
            "Epoch 286 Loss : 0.009816611185669899\n",
            "Epoch 287 Accuracy : 99.05288082083662\n",
            "Epoch 287 Loss : 0.009791512973606586\n",
            "Epoch 288 Accuracy : 99.01341752170481\n",
            "Epoch 288 Loss : 0.009765282273292542\n",
            "Epoch 289 Accuracy : 99.01341752170481\n",
            "Epoch 289 Loss : 0.009739691391587257\n",
            "Epoch 290 Accuracy : 99.01341752170481\n",
            "Epoch 290 Loss : 0.00971272960305214\n",
            "Epoch 291 Accuracy : 99.01341752170481\n",
            "Epoch 291 Loss : 0.009685776196420193\n",
            "Epoch 292 Accuracy : 99.05288082083662\n",
            "Epoch 292 Loss : 0.009660574607551098\n",
            "Epoch 293 Accuracy : 99.01341752170481\n",
            "Epoch 293 Loss : 0.009633468464016914\n",
            "Epoch 294 Accuracy : 99.01341752170481\n",
            "Epoch 294 Loss : 0.009608387015759945\n",
            "Epoch 295 Accuracy : 99.01341752170481\n",
            "Epoch 295 Loss : 0.009582854807376862\n",
            "Epoch 296 Accuracy : 99.05288082083662\n",
            "Epoch 296 Loss : 0.009557241573929787\n",
            "Epoch 297 Accuracy : 99.05288082083662\n",
            "Epoch 297 Loss : 0.009530698880553246\n",
            "Epoch 298 Accuracy : 99.05288082083662\n",
            "Epoch 298 Loss : 0.009504100307822227\n",
            "Epoch 299 Accuracy : 99.05288082083662\n",
            "Epoch 299 Loss : 0.00947754830121994\n",
            "Epoch 300 Accuracy : 99.05288082083662\n",
            "Epoch 300 Loss : 0.00945155881345272\n",
            "\n",
            "Final test loss-\n",
            "0.9858044164037855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
        "y_2 = np.round(regr.predict(X_test))\n",
        "k2= (sum(y_2==y_test.to_numpy())/len(y_test))\n",
        "print(\"nn accuracy \"+str(k2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00K8MsjUFwAV",
        "outputId": "3bda6cad-d908-4faf-dfcd-1d3e65a03bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1599: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nn accuracy 0.9810725552050473\n"
          ]
        }
      ]
    }
  ]
}